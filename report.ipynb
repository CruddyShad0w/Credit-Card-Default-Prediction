{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPfnWsVwmZtk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqpBXMnA1t4U"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPVljS2mmZt2"
   },
   "source": [
    "# Predicting Credit Default.\n",
    "#### Identifying risky credit patterns\n",
    "\n",
    "We'll be analyzing credit card data, collected in Taiwan (link below). Trying to predict patterns that lead to defaulting on credit. Various Machine Learning methods will be applied to the dataset. We will also try to derive some features to help our predictions. The 'real world' applications of these methods will be considered throughout our report. Balancing accuracy and the rate of true positive predictions will be the main consideration throughout the report. \n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEnm1Mb6mZt7"
   },
   "source": [
    "### Definitions:\n",
    "**Default:** After you’ve failed to make a payment on your credit card for 180 days, your issuer assumes you’re probably never going to. At this point, the issuer can (and usually does) close your card, write off what you owe as bad debt and sell your account to a collections agency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1539810817639,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "4BM_SrRSmZuA",
    "outputId": "10e855f0-8c98-411c-b1e2-c552e9961290",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"default_cc_train.csv\", sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note, that as these data were collected in Taiwan, the monetary amounts are all in Tawianese New Dollars. Current conversion rate is approximately 30 TWD to 1 USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1099,
     "status": "ok",
     "timestamp": 1539810819367,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "rgprmh7ymZuS",
    "outputId": "90378561-ac9c-4c8b-b437-42f71c942151",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "mask = np.zeros_like(corr,dtype=np.bool)\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True,\n",
    "            linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.title('Correlation of features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correlation plot we can see that the features most strongly linked with default status are the payment fields. The payment field is categorical, despite having numeric placeholders. The assumption here would be that a higher values (indicating late payments) would correlate very highly with ones likelihood of defaulting on their balance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1539810829003,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "VYQM5WzPmZui",
    "outputId": "bd4c18c4-7fb3-4bb4-c30d-1b2896e94478"
   },
   "outputs": [],
   "source": [
    "df.info()\n",
    "print(\"Shape: \",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-f9OugqymZuv"
   },
   "source": [
    "Looks like its a full 25000 rows with no null values. Does not look like any data cleaning will be needed only scaling and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1539810833908,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "8Rn3dWSgmZuy",
    "outputId": "23d803a1-1dfe-45bb-d0fa-bcb44f46b116",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.countplot(df['default.payment.next.month'])\n",
    "plt.title('Histogram of payment defaults')\n",
    "plt.xticks((0,1),('Default','Good Standing'))\n",
    "plt.xlabel('Default payments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1539810838824,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "uC2HLzl_mZu9",
    "outputId": "8a599377-f07b-4e56-ead2-b3088f5e72b6"
   },
   "outputs": [],
   "source": [
    "print( 'Percentage of default: '+ str(100*len(df[df['default.payment.next.month'] ==1]) / len(df) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VpT8lR5mZvS"
   },
   "source": [
    "About one in every 3.5-4 people will end up defaulting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1539810839669,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "jsuAEjCjmZva",
    "outputId": "085aabda-8ab0-4256-e308-2e11ce2cd469"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['AGE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IX8Zh5_ZmZvq"
   },
   "source": [
    "Interesting distribution here, large concentrations around 28, 35, 43, and 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1539810840987,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "ngzIoUDumZvv",
    "outputId": "3b8e67e3-0df0-4891-c7dc-d6cabfc91fcd"
   },
   "outputs": [],
   "source": [
    "sns.countplot('EDUCATION',hue='default.payment.next.month',data=df)\n",
    "plt.title('Educational Distribution')\n",
    "plt.xlabel('Maximum Level of Education')\n",
    "L = plt.legend(title='Default')\n",
    "L.get_texts()[0].set_text('No')\n",
    "L.get_texts()[1].set_text('Yes')\n",
    "plt.xticks((0,1,2,3,4,5,6),('<12 grade','graduate school','university','high school','other','trade school','Not disclosed'),rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntx_bkVsmZv5"
   },
   "source": [
    "Our debtors are mostly educated people. There appears to be no significance in the relationship between education and defaulting on your credit card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1539810845007,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "wQ9isD9vmZv7",
    "outputId": "dc12c172-c25c-4c65-c1cc-b4d03afdd098"
   },
   "outputs": [],
   "source": [
    "#mar_map = {0:'Not Provided',1:'Married',2:'Single',3:'Unknown'}\n",
    "#mar_status = [mar_map[stat] for stat in df['MARRIAGE']]\n",
    "sns.countplot('MARRIAGE', hue='default.payment.next.month',data=df)\n",
    "plt.xticks((0,1,2,3),('Not Provided','Married','Single','Unknown'),rotation=45)\n",
    "L = plt.legend(title='Default')\n",
    "L.get_texts()[0].set_text('No')\n",
    "L.get_texts()[1].set_text('Yes')\n",
    "plt.title('Relationship Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpAb4gfgmZwN"
   },
   "source": [
    "A greater percentage of married people end up defaulting on their debt. One would suspect otherwise, with the additional responsibility and future planning that usually comes with marriage. But perhaps single people have fewer financial burdens, like children, and are less likely to overextend themselves financially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1608,
     "status": "ok",
     "timestamp": 1539810846825,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "iYGaCIJQmZwR",
    "outputId": "e1557018-cc94-4c47-f3d8-abfaf1f82f07"
   },
   "outputs": [],
   "source": [
    "plot = sns.countplot(df['LIMIT_BAL'],hue = df['default.payment.next.month'])\n",
    "plot.set_xticks(plot.get_xticks()[::5])\n",
    "plot = plt.xticks(rotation=90)\n",
    "plt.title('Histogram of Credit Limit Balances')\n",
    "L = plt.legend(title='Default')\n",
    "L.get_texts()[0].set_text('No')\n",
    "L.get_texts()[1].set_text('Yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fc6f2p7NmZwZ"
   },
   "source": [
    "Kind of a hard figure to see but shows the relationship between a persons balance limit and their default. Generally a higher percentage of people with lower limit balances will end up defaulting. If you're a 'high risk' applicant the bank usually will only approve you for a smaller line of credit. This graph might help show why that is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1539810847674,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "V3FVGS92mZwe",
    "outputId": "0f54daf1-ded1-4fcc-c8cd-9f1868252c31"
   },
   "outputs": [],
   "source": [
    "default_labs = {0:'Good Standing',1:'Default'}\n",
    "default = [default_labs[x] for x in df['default.payment.next.month']]\n",
    "sns.violinplot(x=default, y='LIMIT_BAL',data=df)\n",
    "plt.title(\"Limit Balance and default distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEygpdQImZwp"
   },
   "source": [
    "Perhaps not very surprisingly, people with higher limit balances are less likely to to be defaulting. The defaulters had a lower average credit limit. Likely they already had a lower credit score to begin with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1539810848857,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "mfpgQqUzmZws",
    "outputId": "fee83aba-f293-4393-b058-1d4616f6f477"
   },
   "outputs": [],
   "source": [
    "sns.violinplot('default.payment.next.month', y='AGE',data=df)\n",
    "plt.title('Default behavior by age distribution')\n",
    "plt.xticks((0,1),('Default','Good Standing'))\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7-55YVkmZxQ"
   },
   "source": [
    "Almost no difference at all in age distributions between defaulters and not. A little surprising actually. One would imagine younger people defaulting more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1aXPoNSmZxX"
   },
   "source": [
    "#### Bill amounts\n",
    "According to the correlation plot above, the most significant features for predicting default payments will be the BILL_AMT fields. Let's visualize a couple to see what we can infer from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1539810850316,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "o5UFmPu3mZxb",
    "outputId": "d7a94dc2-eb5f-488f-c104-7129bd5c2b59"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['BILL_AMT1'])\n",
    "plt.title('Distribution of Bill Amounts in month 1')\n",
    "plt.xlabel('Money Owed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTUqPjuDmZxs"
   },
   "source": [
    "A very steep distribution toward the lowered end of money owed. Is this the general pattern this data will follow? Lets look at another month to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1539810851624,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "QKMkOJGsmZxu",
    "outputId": "c38f5577-b990-4812-d42b-4cccb475124c"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['BILL_AMT6'])\n",
    "plt.title('Distribution of Bill Amounts in month 6')\n",
    "plt.xlabel('Money Owed')\n",
    "plt.xlim((-50000,600000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5xS9h9kmZyA"
   },
   "source": [
    "Almost exactly the same. This seems to make sense. Most people won't want to have too much debt. The vast majority of people have debt below about 50,000 Taiwan New Dollars (about $1650 USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1539810853286,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "3d8_GLbEmZyH",
    "outputId": "a4042f9e-b5ae-4bf5-935d-81e29e765471"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='PAY_2', hue = 'default.payment.next.month' , data = df)\n",
    "plt.title('Defaults by payment status')\n",
    "plt.xlabel('Payment Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4CQHm_SXmZyb"
   },
   "source": [
    "Not everyone that pays on time, or is paid up in a given month is guaranteed not to default. But for the folks that are already a few months late, it looks like you're more likely to end up in default than you are to end up paying off your debt. As we saw earlier in the correlation plot, payment status will be a very strong predictor of default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSTHDhk9mZyd"
   },
   "source": [
    "### Derived Features: \n",
    "#### To add or not to add?\n",
    "Our team played with a few strategies to synthesize some more features. Some features tested were: ratio of money owed to total balance, sum off amounts owed across each month, as well as a couple others trying to get something more meaningful out of the 'limit balance', 'pay amount' and 'bill amount'. Below is some feature engineering we've done, and through this we've managed to bump our average predictions up a couple %. No revolutionary gains, but something is better than nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IAEg6_K2gus"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer class for the\n",
    "    [default of credit card clients]\n",
    "    (https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)\n",
    "Data Set.\n",
    "\n",
    "Usage:\n",
    "    import preprocessing\n",
    "    transformer = preprocessing.Transformer()\n",
    "    X_train = transformer.fit_transform(df_train)\n",
    "    X_test = transformer.transform(df_test)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def _get_cols(prefix, from_idx, to_idx):\n",
    "    res = []\n",
    "    for i in range(from_idx, to_idx + 1):\n",
    "        res.append(prefix + str(i))\n",
    "    return res\n",
    "\n",
    "\n",
    "class FeatureEngineer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    A Transformer for Feature Enginnering\n",
    "    \"\"\"\n",
    "    PAY_AMTS = _get_cols('PAY_AMT', 1, 6)\n",
    "    BILL_AMTS = _get_cols('BILL_AMT', 1, 6)\n",
    "    PAYS = ['PAY_0'] + _get_cols('PAY_', 2, 6)\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        \"\"\"\n",
    "        :type dataframe pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        \"\"\"\n",
    "        :type dataframe pandas.DataFrame\n",
    "        \"\"\"\n",
    "        dataframe['TOT_PAY_AMT'] = self._tot_pay_amt(dataframe)\n",
    "        dataframe['TOT_BILL_AMT'] = self._tot_bill_amt(dataframe)\n",
    "        dataframe['TOT_MOD'] = self._tot_month_of_delays(dataframe)\n",
    "        dataframe['NUM_NO_COMSP'] = self._num_of_no_consumption(dataframe)\n",
    "        dataframe['NUM_NO_DLY'] = self._num_of_no_delay(dataframe)\n",
    "        return dataframe\n",
    "\n",
    "    def _tot_pay_amt(self, dataframe):\n",
    "        return dataframe[self.PAY_AMTS].sum(axis=1)\n",
    "\n",
    "    def _tot_bill_amt(self, dataframe):\n",
    "        return dataframe[self.BILL_AMTS].sum(axis=1)\n",
    "\n",
    "    def _tot_month_of_delays(self, dataframe):\n",
    "        return dataframe[self.PAYS].replace([-2, -1], 0).sum(axis=1)\n",
    "\n",
    "    def _num_of_no_consumption(self, dataframe):\n",
    "        return (dataframe[self.PAYS] == -2).sum(axis=1)\n",
    "\n",
    "    def _num_of_no_delay(self, dataframe):\n",
    "        return (dataframe[self.PAYS] == -1).sum(axis=1)\n",
    "\n",
    "\n",
    "class NumFeatureSelector(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Select Numeric Features\n",
    "    \"\"\"\n",
    "    FEATURES_ = [\n",
    "        'LIMIT_BAL', 'AGE', 'TOT_PAY_AMT', 'TOT_BILL_AMT', 'TOT_MOD',\n",
    "        'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5',\n",
    "        'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4',\n",
    "        'PAY_AMT5', 'PAY_AMT6'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        \"\"\"\n",
    "        :type dataframe pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        \"\"\"\n",
    "        :type dataframe pandas.DataFrame\n",
    "        \"\"\"\n",
    "        return dataframe[self.features].values\n",
    "\n",
    "\n",
    "class CatFeatureSelector(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Select Categorical Features\n",
    "    \"\"\"\n",
    "    FEATURES_ = [\n",
    "        'SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0',\n",
    "        'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "        'NUM_NO_COMSP', 'NUM_NO_DLY'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, features=None):\n",
    "        super().__init__()\n",
    "        self._encoder = None\n",
    "        if features is None:\n",
    "            features = self.CAT_FEATURES_\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        \"\"\"\n",
    "        :type dataframe pandas.DataFrame\n",
    "        \"\"\"\n",
    "        self._encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        X = dataframe[self.features].astype('int32')\n",
    "        X -= X.min().min()\n",
    "        self._encoder.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        \"\"\"\n",
    "        :type dataframe pandas.DataFrame\n",
    "        \"\"\"\n",
    "        X = dataframe[self.features].astype('int32')\n",
    "        X -= X.min().min()\n",
    "        return self._encoder.transform(X).toarray()\n",
    "\n",
    "    def feature_indices(self):\n",
    "        return self._encoder.feature_indices_\n",
    "\n",
    "\n",
    "class Transformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transform the `default of credit card clients` data\n",
    "    \"\"\"\n",
    "    _feature_enginner = FeatureEngineer()\n",
    "\n",
    "    def __init__(self, num_features=None, cat_features=None):\n",
    "        super().__init__()\n",
    "        self._need_to_fit = True\n",
    "        self._pipe = None\n",
    "        self.df = None\n",
    "        self.cat_selector = None\n",
    "        if num_features is None:\n",
    "            num_features = NumFeatureSelector.FEATURES_\n",
    "        if cat_features is None:\n",
    "            cat_features = CatFeatureSelector.FEATURES_\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "\n",
    "    def fit(self, dataframe):\n",
    "        \"\"\"\n",
    "        :type dataframe pandas.DataFrame\n",
    "        \"\"\"\n",
    "        self._need_to_fit = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        \"\"\"\n",
    "        :type dataframe pandas.DataFrame\n",
    "        \"\"\"\n",
    "        self.df = self._feature_enginner.fit_transform(dataframe)\n",
    "        if self._need_to_fit:\n",
    "            self._need_to_fit = False\n",
    "            self.cat_selector = CatFeatureSelector(self.cat_features)\n",
    "            self._pipe = Pipeline([\n",
    "                ('Feature Union', FeatureUnion([\n",
    "                    ('Numeric Features', NumFeatureSelector(self.num_features)),\n",
    "                    ('Encoded Categorical Features', self.cat_selector),\n",
    "                ])),\n",
    "                ('Standard Scaler', StandardScaler()),\n",
    "            ])\n",
    "            return self._pipe.fit_transform(self.df)\n",
    "        return self._pipe.transform(self.df)\n",
    "\n",
    "    def feature_importances(self, importances):\n",
    "        \"\"\"\n",
    "        :type importances numpy.ndarray\n",
    "        \"\"\"\n",
    "        indices = self.cat_selector.feature_indices()\n",
    "        col = len(self.num_features)\n",
    "        cat_scores = np.zeros(len(self.cat_features))\n",
    "        for i in range(1, len(indices)):\n",
    "            cat_scores[i - 1] = np.sum(importances[\n",
    "                (col + indices[i - 1]):(col + indices[i])\n",
    "            ])\n",
    "        return pd.DataFrame({\n",
    "            'feature': self.num_features + self.cat_features,\n",
    "            'importance': np.concatenate((\n",
    "                importances[:len(self.num_features)],\n",
    "                cat_scores\n",
    "            ))\n",
    "        }).sort_values(by=['importance'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2jRdkqiU9Fs"
   },
   "source": [
    "The `Transformer` class wraps the entire preprocessing logic. It contains feature engineering, selecting numeric and categorical features and transform them into indicator variables. It also provides a utility method to restore the original feature importance for the categorical features.\n",
    "\n",
    "The new features are \n",
    "\n",
    "`TOT_PAY_AMT`: Total payment in the 6 months.\n",
    "\n",
    "`TOT_BILL_AMT`: Total payment in the 6 months.\n",
    "\n",
    "`TOT_MOD`: Number of the month of delay. \n",
    "\n",
    "`NUM_NO_COMSP`: Number of months without consumption (with pay value of `-2`)\n",
    "\n",
    "`NUM_NO_DLY`: Number of months paid on time in full (with pay value of `-1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMKOWoBD4mtS"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer()\n",
    "X = transformer.fit_transform(df)\n",
    "df = transformer.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1539810959862,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "Em-OeURx4pXr",
    "outputId": "37bd7e45-eadd-46f5-92ad-320237aa8f5a"
   },
   "outputs": [],
   "source": [
    "def get_color(x):\n",
    "    return 'b' if x == 0 else 'r'\n",
    "\n",
    "df[df['TOT_BILL_AMT'] <= 0]['default.payment.next.month'].plot(kind='hist', density=1, alpha=.3)\n",
    "df[df['TOT_BILL_AMT'] > 0]['default.payment.next.month'].plot(kind='hist', density=1, alpha=.3)\n",
    "plt.legend(['Total Bill Amount <= 0', 'Total Bill Amount > 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZQV3wvLcVGyl"
   },
   "source": [
    "We found people with negative total bill amount are more likely to default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzSO31Fh_DHv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['TOT_PAY_AMT'], df['TOT_BILL_AMT'], s=2, alpha=0.5, c=list(map(get_color, df['default.payment.next.month'])))\n",
    "plt.xlabel('Total Payment')\n",
    "plt.ylabel('Total Bill')\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the higher the ratio of total bill amount to your total payments, the more likely one is to default. This is completely logical, you're paying less than you owe, this leads to the bank closing your line of credit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDyxkKjL_EWF"
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['TOT_MOD'], df['TOT_BILL_AMT'], s=2, alpha=0.5, c=list(map(get_color, df['default.payment.next.month'])))\n",
    "plt.xlabel('Total Month of Delays')\n",
    "plt.ylabel('Total Bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0kDovFNVNia"
   },
   "source": [
    "The total month of delays is the sum of the `PAY` columns excluding the negative values. This new feature indicates people with high total month of delays are more likely to default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMADFxVxmZyf"
   },
   "source": [
    "# Trying classifiers\n",
    "We will test out a broad suite of Machine Learning algorithms, as well as a few ensemble methods to find the best means of predicting defaulters. A meta class Selector will be used to track and manage the results of the different algorithms to be sorted after all tests have been run.\n",
    "In our analysis we will be focusing mainly on Accuracy, and the True Positive prediction rate, as it's most important for us to be predicting who would be defaulting, thus costing the bank/credit agency money. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLIC7h74mZyh"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import  VotingClassifier,GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor,LinearRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlNA5WMymZyl"
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "X = transformer.fit_transform(df)\n",
    "y = df['default.payment.next.month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5009,
     "status": "ok",
     "timestamp": 1539810979872,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "XMBLihJv3EOl",
    "outputId": "0694d64b-474c-4a1c-f2e5-05621ac6b589",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_frst = RandomForestClassifier(n_estimators=100).fit(X, y)\n",
    "transformer.feature_importances(rand_frst.feature_importances_).plot(kind='bar', x='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before transforming the data we see the features that are most important to predicting default are `MARRIAGE`, `AGE` and `LIMIT_BAL`. These are all \"vanilla\" features. But will this remain the case after transforming the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJup6Ozx3FuE"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(cat_features=[\n",
    "    'SEX', 'MARRIAGE', 'PAY_0'\n",
    "])\n",
    "X = transformer.fit_transform(df)\n",
    "y = df['default.payment.next.month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5248,
     "status": "ok",
     "timestamp": 1539811000942,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "obbzGlLB3LkE",
    "outputId": "ca7a5214-049c-442d-9566-e597e790ca16"
   },
   "outputs": [],
   "source": [
    "rand_frst = RandomForestClassifier(n_estimators=100).fit(X, y)\n",
    "transformer.feature_importances(rand_frst.feature_importances_).plot(kind='bar', x='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjNo-oT5VS1g"
   },
   "source": [
    "One of the derived feature `TOT_MOD` has high feature importance value according to a random forest classifier.\n",
    "We remove the categorical variables with low feature importance value to reduce the dimensionality and improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BuOPDwLmZzB"
   },
   "source": [
    "### ML Specific Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZ5NTELU3YeM"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import \\\n",
    "        GridSearchCV, \\\n",
    "        learning_curve, \\\n",
    "        train_test_split\n",
    "from sklearn.metrics import \\\n",
    "        accuracy_score, \\\n",
    "        confusion_matrix, \\\n",
    "        roc_curve, \\\n",
    "        precision_recall_fscore_support\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, name, base_estimator, param_grid):\n",
    "        \"\"\"\n",
    "        A model is constructed by `ModelSelection.search_and_test`\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.base_estimator = base_estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.id = None\n",
    "        self.best_estimator = None\n",
    "        self.best_params = None\n",
    "        self.train_score = None\n",
    "        self.accuracy = None\n",
    "        self.precision = None\n",
    "        self.recall = None\n",
    "        self.fbeta = None\n",
    "        self.support = None\n",
    "        self.tn = None\n",
    "        self.tp = None\n",
    "        self.fn = None\n",
    "        self.fp = None\n",
    "        \n",
    "\n",
    "    def as_df(self):\n",
    "        return pd.DataFrame({\n",
    "            'id': [self.id],\n",
    "            'name': [self.name],\n",
    "            'best_params': [self.best_params],\n",
    "            'train_score': [self.train_score],\n",
    "            'accuracy': [self.accuracy],\n",
    "            'precision': [self.precision],\n",
    "            'recall': [self.recall],\n",
    "            'fbeta': [self.fbeta],\n",
    "            'support': [self.support],\n",
    "            'tn': [self.tn],\n",
    "            'tp': [self.tp],\n",
    "            'fn': [self.fn],\n",
    "            'fp': [self.fp]\n",
    "        }).set_index('id')\n",
    "\n",
    "\n",
    "class ModelSelection:\n",
    "    TARGET_ = 'default.payment.next.month'\n",
    "\n",
    "    def __init__(self, df, transformer, test_size, max_parallelism=None):\n",
    "        df_train, df_test = train_test_split(df, test_size=test_size)\n",
    "        self.X_train = transformer.fit_transform(df_train)\n",
    "        self.y_train = df_train[self.TARGET_]\n",
    "        self.X_test = transformer.transform(df_test)\n",
    "        self.y_test = df_test[self.TARGET_]\n",
    "        self.models = []\n",
    "        self.max_parallelism = max_parallelism\n",
    "\n",
    "    def search_and_test(self, name, base_estimator, param_grid, cv=5, verbose=1):\n",
    "        \"\"\"\n",
    "        Perform Grid Search on the param grid & Test the best model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name: string\n",
    "\n",
    "        base_estimator: sklearn.estimator\n",
    "\n",
    "        param_grid: dict\n",
    "            param_grid for GridSearch\n",
    "\n",
    "        cv: int, cross-validation generator or an iterable, optional\n",
    "        \"\"\"\n",
    "        model = Model(name, base_estimator, param_grid)\n",
    "        gs = GridSearchCV(base_estimator, param_grid=param_grid,\n",
    "                          cv=cv, n_jobs=self.max_parallelism, verbose=verbose)\n",
    "        if verbose != 0:\n",
    "            print('Starting Grid Search')\n",
    "        gs.fit(self.X_train, self.y_train)\n",
    "        model.best_params = gs.best_params_\n",
    "        model.best_estimator = gs.best_estimator_\n",
    "        model.train_score = gs.best_score_\n",
    "        if verbose != 0:\n",
    "            print('Fitting the best estimator')\n",
    "        model.best_estimator.fit(self.X_train, self.y_train)\n",
    "        if verbose != 0:\n",
    "            print('Testing the best estimator')\n",
    "        y_pred = model.best_estimator.predict(self.X_test)\n",
    "        model.accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        cm = confusion_matrix(self.y_test, y_pred)\n",
    "        model.tn, model.fp, model.fn, model.tp = cm.ravel()\n",
    "        scores = precision_recall_fscore_support(\n",
    "            self.y_test, y_pred, average='binary'\n",
    "        )\n",
    "        model.precision, model.recall, model.fbeta, model.support = scores\n",
    "        model.id = len(self.models)\n",
    "        self.models.append(model)\n",
    "        return model\n",
    "\n",
    "    def plot_confusion_matrix(self, ax, model, normalize=True):\n",
    "        \"\"\"\n",
    "        Plots the confusion matrix for the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Model\n",
    "            A model should be contructed by `search_and_test` method\n",
    "        \"\"\"\n",
    "        df_cm = pd.DataFrame({\n",
    "            0: [model.tn, model.fn],\n",
    "            1: [model.fp, model.tp]\n",
    "        })\n",
    "        if normalize:\n",
    "            total = model.tn + model.tp + model.fn + model.fp\n",
    "            df_cm = df_cm.astype('float') / total\n",
    "        sns.heatmap(df_cm, annot=True, ax=ax)\n",
    "        ax.set_title('Confusion Matrix\\n{}\\n{}'.format(\n",
    "            model.name, model.best_params\n",
    "        ))\n",
    "        ax.set_ylabel(\"True\")\n",
    "        ax.set_xlabel(\"Prediction\")\n",
    "        return plt\n",
    "\n",
    "    def plot_learning_curve(self, ax, model, ylim=None, cv=None,\n",
    "                            train_sizes=np.linspace(.5, 1.0, 5)):\n",
    "        \"\"\"\n",
    "        Generate a simple plot of the test and training learning curve.\n",
    "        http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Model\n",
    "            A model should be contructed by `search_and_test` method\n",
    "\n",
    "        ylim : tuple, shape (ymin, ymax), optional\n",
    "            Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "        cv : int, cross-validation generator or an iterable, optional\n",
    "\n",
    "        train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "            Relative or absolute numbers of training examples that will be used to\n",
    "            generate the learning curve. If the dtype is float, it is regarded as a\n",
    "            fraction of the maximum size of the training set (that is determined\n",
    "            by the selected validation method), i.e. it has to be within (0, 1].\n",
    "            Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        \"\"\"\n",
    "        ax.set_title(\"{}\\n{}\".format(model.name, model.best_params))\n",
    "        if ylim is not None:\n",
    "            ax.set_ylim(*ylim)\n",
    "        ax.set_xlabel(\"Training examples\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            model.best_estimator, self.X_train, self.y_train, cv=cv,\n",
    "            n_jobs=self.max_parallelism, train_sizes=train_sizes)\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "        ax.grid()\n",
    "\n",
    "        ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "        ax.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        ax.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "        ax.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "\n",
    "        ax.legend(loc=\"best\")\n",
    "        return plt\n",
    "\n",
    "    def get_predictions(self, model):\n",
    "        '''\n",
    "        Get the predictions from a model given multiple parameters to test\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        model: Used for training to get predictions\n",
    "        '''\n",
    "        flat = [[(k, v) for v in vs] for k, vs in model.param_grid.items()]\n",
    "        parameters_flat = [dict(items) for items in product(*flat)]\n",
    "   \n",
    "        predictions = []\n",
    "        for i in parameters_flat:\n",
    "            model.base_estimator.set_params(**i)\n",
    "            model.base_estimator.fit(self.X_train, self.y_train)\n",
    "            predict = model.base_estimator.predict(self.X_test)\n",
    "            predictions.append(predict)\n",
    "        return predictions, parameters_flat\n",
    "    \n",
    "    def plot_PR(self, ax, model, predictions, parameters_flat):\n",
    "        '''\n",
    "        Generates a Precision Recall plot for our model given a set of parameters to search through\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model:Used for getting the name, and best aprameters for title\n",
    "        \n",
    "        predictions : An array of predictions from a model given multiple parameters to test\n",
    "        \n",
    "        parameters_flat: a set of all the possible parameters for testing.\n",
    "        '''\n",
    "        model_name = list(range(0,400))\n",
    "        count = 0\n",
    "        a = np.empty((1,4))\n",
    "        for i in predictions:\n",
    "            precision, recall, f, beta = precision_recall_fscore_support(self.y_test, i, average='binary')\n",
    "            a = np.append(a, np.array([[model_name[count], parameters_flat[count], precision, recall]]),axis = 0)\n",
    "            plt.scatter(precision, recall)\n",
    "            count = count + 1\n",
    "        ax.set_xlabel(\"precision\")\n",
    "        ax.set_ylabel(\"recall\")\n",
    "        ax.set_title('Precision Recall\\n{}'.format(model.name))\n",
    "\n",
    "        legend = pd.DataFrame(a[1:])\n",
    "        legend.columns = ['model#', 'parameters','precision','recall']\n",
    "        #return(legend)\n",
    "    \n",
    "    def plot_ROC(self, ax, model, predictions, parameters_flat):\n",
    "        \"\"\"\n",
    "        Generates a ROC scatter plot for our model given a set of parameters to search through\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model:Used for getting the name, and best aprameters for title\n",
    "        \n",
    "        predictions : An array of predictions from a model given multiple parameters to test\n",
    "        \n",
    "        parameters_flat: a set of all the possible parameters for testing.\n",
    "        \"\"\"\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        model_name = list(range(0,600))\n",
    "        a = np.empty((1,4))\n",
    "\n",
    "        count = 0\n",
    "        for i in predictions: \n",
    "            fpr, tpr, thresholds = roc_curve(self.y_test, i)      \n",
    "            ax.scatter(fpr, tpr)\n",
    "            a = np.append(a, np.array([[model_name[count], parameters_flat[count], fpr, tpr]]),axis = 0)\n",
    "            count = count + 1\n",
    "        \n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC\\n{}'.format(model.name))\n",
    "        \n",
    "        legend = pd.DataFrame(a[1:])\n",
    "        legend.columns = ['model#', 'parameters','fpr','tpr']\n",
    "        #return(legend)\n",
    "    \n",
    "    def plot_ROC_PR(self, axes, model):\n",
    "        '''\n",
    "        Plot the Precision Recall and Receiver Operating curves\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        model: stores the values of parameters and predictions\n",
    "            \n",
    "        '''    \n",
    "        predictions, parameters = self.get_predictions(model)\n",
    "        legend1 = self.plot_ROC(axes[0], model, predictions, parameters)\n",
    "        legend2 = self.plot_PR(axes[1], model, predictions, parameters)\n",
    "        legends = [legend1,legend2]\n",
    "        #return legends \n",
    "\n",
    "    def plot(self, model, verbose=1):\n",
    "        \"\"\"\n",
    "        Generates all the plots we have for the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Model\n",
    "            A model should be contructed by `search_and_test` method\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "        plt.subplots_adjust(top=1.05, hspace=0.3)\n",
    "        if verbose != 0:\n",
    "            print('Plotting the Confusion Matrix')\n",
    "        self.plot_confusion_matrix(axes[0][0], model)\n",
    "        if verbose != 0:\n",
    "            print('Plotting the Learning Curve', )\n",
    "        self.plot_learning_curve(axes[1][0], model)\n",
    "        if verbose != 0:\n",
    "            print('Plotting the Precision vs Recall')\n",
    "        self.plot_ROC_PR((axes[0][1], axes[1][1]), model)\n",
    "\n",
    "\n",
    "    def as_df(self):\n",
    "        \"\"\"\n",
    "        Output the models as a pandas dataframe\n",
    "        \"\"\"\n",
    "        return pd.concat(\n",
    "            [model.as_df() for model in self.models]\n",
    "        ).sort_values(by=['accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ModelSelection(df, transformer, test_size=0.3, max_parallelism=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VdNaMJZdmZzY"
   },
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1837753,
     "status": "ok",
     "timestamp": 1539815365417,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "XeL5kNVamZzc",
    "outputId": "7e5f17fa-d03b-432b-b80c-bec776176e41"
   },
   "outputs": [],
   "source": [
    "gb = selector.search_and_test(\n",
    "    'Gradient Boosting Classifier',\n",
    "    GradientBoostingClassifier(),\n",
    "    {\n",
    "        'n_estimators': range(1, 20, 2),\n",
    "        'max_depth': range(1, 10, 2),\n",
    "        'learning_rate': [0.01, 0.1, 1]\n",
    "    }\n",
    ")\n",
    "selector.plot(gb)\n",
    "gb.as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that gradient boosting should be a good model to choose for our final voting classifier. We can see that its TP rate is good given the data set. We can also see that this classifier will perform better given a larger dataset. Looking at the ROC curve, we can see that the TP rate plateaus around 0.4 The precision-recall seems fairly good given it has a high precision given a relatively low recall. All of these characteristics make this a good candidate for our voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38aTFc0vmZz7"
   },
   "source": [
    "## Stochastic Gradient Desecent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1235092,
     "status": "ok",
     "timestamp": 1539816602437,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "zRm3dyV1mZz8",
    "outputId": "50323b3f-7597-4394-c0d0-54a9fedbce90"
   },
   "outputs": [],
   "source": [
    "sgd = selector.search_and_test(\n",
    "    'Stochastic Gradient Desecent Classifier',\n",
    "    SGDClassifier(),\n",
    "    {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1],\n",
    "        'loss': ['hinge', 'modified_huber', 'log'],\n",
    "        'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "        'class_weight': [\n",
    "            {0: 1, 1: i} for i in np.linspace(0.25, 2.5, 5)\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "selector.plot(sgd)\n",
    "sgd.as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent performed better than our previous classifier in terms of TP to FP rate. Although it performed well we can see that this model will not improve given more data because it suffers from high bias. Looking at the ROC plot we can see that the TP rate increases until it develops an elbow. This is where the model is predicting the best estimator. We can also see from our precision-recall that our model does fairly well keeping high precision with a relatively low recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXfkNDaVmZ0Y"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MR3KFMeomZ0Z"
   },
   "outputs": [],
   "source": [
    "svc = selector.search_and_test(\n",
    "    'Support Vector Machine Classifier',\n",
    "    svm.SVC(),\n",
    "    {\n",
    "        'kernel': ['rbf', 'sigmoid'],\n",
    "        'C': [0.01, 0.1, 1],\n",
    "        'class_weight': [\n",
    "            {0: 1, 1: i} for i in np.linspace(0.25, 2.5, 5)\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "selector.plot(svc)\n",
    "svc.as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine is our best performing model so far. It has the highest TP rate of all the models so far. We can see that this model is not converging in the learning curve meaning more data will not help us. Looking at the ROC, we can see that we are achieving relatively similar TP rates with an increased FP rate compared to previous models. Our Precision-recall curve seems to be more dispersed, but we are still focused on the highest precision and lowest recall picking the model around .67 precision, 0.37 recall. This model has a high TP classification making it a good candidate for our final voting classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "capqakdfmZ0u"
   },
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 155
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 331364,
     "status": "ok",
     "timestamp": 1539816934839,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "p44bXpytmZ0w",
    "outputId": "2b442522-2751-463a-8b10-a81c5b6fce8d"
   },
   "outputs": [],
   "source": [
    "knn = selector.search_and_test(\n",
    "    'K-Nearest Neighbors Classifier',\n",
    "    KNeighborsClassifier(),\n",
    "    {\n",
    "        'n_neighbors': range(1, 11, 2)\n",
    "    }\n",
    ")\n",
    "selector.plot(knn)\n",
    "knn.as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is performing relatively well. We can see this model has high variance in it but from the ROC we can see the TP rate is good enough to possibly be included in our final model. From looking at our Precision-Recall curve we can see we have chosen the model with a precision of around .6 and a recall of about .37 which is similar to our SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhlfiO9nmZ0-"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20561,
     "status": "ok",
     "timestamp": 1539816956503,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "7a0EO813mZ1A",
    "outputId": "82cb2e39-c64f-483b-c879-94771cca578b"
   },
   "outputs": [],
   "source": [
    "dt = selector.search_and_test(\n",
    "    'Decision Tree Classifier',\n",
    "    DecisionTreeClassifier(),\n",
    "    {\n",
    "        'max_depth': range(1, 5),\n",
    "        'max_features': [None, 3, 6, 18],\n",
    "        'class_weight': [\n",
    "            {0: 1, 1: i} for i in np.linspace(0.25, 2.0, 5)\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "selector.plot(dt)\n",
    "dt.as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has the best TP classification rate of all. We can see in the ROC graph that the TP rate has an elbow around 0.6 TP rate which is close to the model we chose. along with this we can see that the Precision and Recall are fairly high at the bottom right portion of the graph. This model looks good except for the learning curve. Given that the Training and CV scores are close to each other we know the model has High Bias. Although it suffers from this we will use this model because of its high TP rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = selector.search_and_test(\n",
    "    \"Random Forest Classifier\",\n",
    "    RandomForestClassifier(),\n",
    "    {\n",
    "        'n_estimators': range(35, 46),\n",
    "        'max_depth': range(3, 13, 2),\n",
    "        'class_weight': [\n",
    "            {0: 1, 1: i} for i in np.linspace(0.25, 2.5, 5)\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "selector.plot(rf)\n",
    "rf.as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is performing fairly well. The TP rate is about average. Looking at our ROC plot, we can see that the elbow occurs around 0.4 TP rate. We can get a higher TP of 0.6, but the cost of TP vs FP becomes too high. From our precision-recall we can see our model is performing well having a relatively high precision with a low recall. This model may be a good candidate for our final model given its high TP rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6DXsCuimZ1e"
   },
   "source": [
    "## ADA-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 364780,
     "status": "ok",
     "timestamp": 1539817656539,
     "user": {
      "displayName": "Kyle Hays",
      "photoUrl": "",
      "userId": "01076447387379991295"
     },
     "user_tz": 420
    },
    "id": "G5TEtQVImZ1g",
    "outputId": "fd1a76ba-e133-4405-b5cc-06489b41e123"
   },
   "outputs": [],
   "source": [
    "ada = selector.search_and_test(\n",
    "    \"ADA Boost Classifier\",\n",
    "    AdaBoostClassifier(dt.best_estimator),\n",
    "    {\n",
    "        'n_estimators': range(10, 100, 10),\n",
    "        'learning_rate': [0.001, 0.01, 1]\n",
    "    }\n",
    ")\n",
    "selector.plot(ada)\n",
    "ada.as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another well-performing model. We can see that its accuracy is high along with its TP total. From examining our ROC plot, we can see the model has a high TP rate with a relatively low range of FP values given how many TP's it is classifying. When looking at our Precision-Recall curve, we can see that out precision is ok with an ok recall. This precision-recall value is likely ok due to the large amount of positive classifications being done. From the learning curve, we can see this model has high bias meaning it will not perform better given more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_sorted_tp = selector.as_df().sort_values(by=['tp'], ascending=False)\n",
    "estimators = []\n",
    "for i in range(0,4):\n",
    "    estimators.append((\n",
    "        selector.models[models_sorted_tp.index[i]].name, \n",
    "        selector.models[models_sorted_tp.index[i]].best_estimator\n",
    "    ))\n",
    "selector.as_df().sort_values(by=['tp'], ascending = False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we created a voting classifier using the top 4 models sorted by TP rate. The classifier is weighted more heavily towards the first classifier in the voter. This classifier will out beat other classifiers 90% of the time with SVM outperforming it the other 10% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "voting = selector.search_and_test(\n",
    "    'Voting Classifier',\n",
    "    VotingClassifier(estimators=estimators),{\n",
    "        'voting': ['hard'], \n",
    "        'weights': [[2, 1, 1, 1],\n",
    "                    [2, 2, 1, 1],\n",
    "                    [1, 2, 1, 1],\n",
    "                    [1, 2, 2, 1],\n",
    "                    [1, 1, 1, 2]]\n",
    "    }\n",
    ")\n",
    "selector.as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we created a voting classifier using the top 4 models sorted by TP rate. The classifier is weighter more heavily towards the first classifier in the voter. This classifier will outbeat other classifiers 90% of the time with SVM out performing it the other 10% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of training and predictions\n",
    "If sorting by `True Positive` rate, the `Stochastic Gradient Descent Classifier` has the best performance.\n",
    "If sorting by pure `Accuracy`, our `Voting Classifier` has the best performance.\n",
    "The range of `Accuracy` was relatively tight, with a minimum of about 80% with `KNearest Neighbors` and a maximum just shy of 82% with the `Voting Classifier`. \n",
    "There was  greater disparity of `True Positive` rates, with `Stochastic Gradient Descent Classifier` getting the maximum score of 623, and the `Decision Tree Classifier` getting a minimum of 456. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = selector.as_df()\n",
    "\n",
    "''' \n",
    "Take the row index of the predictor with the\n",
    "highest acccuracy from the models dataframe\n",
    "''' \n",
    "top_model_id = models.index[0]\n",
    "\n",
    "# Estimator is already instantiated with best hyperparameters\n",
    "best_estimator = selector.models[top_model_id].best_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Enter New Test Data Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Please enter file string for test data to be used in grading report. '''\n",
    "test_data_file_string = ''  # Enter file string of new test data\n",
    "\n",
    "def bundle_predict(estimator, train_dat, test_dat):\n",
    "    '''\n",
    "    Takes in new training and test data, transforms, then trains\n",
    "    and predicts on new training and test sets.\n",
    "\n",
    "    Parameters\n",
    "    ---------------\n",
    "    estimator - best estimator from selector class, parameters tuned.\n",
    "\n",
    "    train_dat - pandas dataframe (the 25K rows of data collected from:\n",
    "        https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients).\n",
    "\n",
    "    test_dat - pandas dataframe ( the new data provided by the grader\n",
    "        of the project, Dr. Bruns).\n",
    "    \n",
    "    '''\n",
    "    TARGET_ = 'default.payment.next.month'\n",
    "    y_train_dat = train_dat[TARGET]\n",
    "    # Fit and transform data\n",
    "    train_dat = transformer.fit_transform(train_dat)\n",
    "    # Transform test data with fit from training data\n",
    "    test_dat = transformer.transform(test_dat)\n",
    "    estimator.fit(train_dat, y_train_dat)\n",
    "    return estimator.predict(test_dat)\n",
    "\n",
    "# Read in ALL data previously used to be used ONLY as training data\n",
    "train_dat = pd.read_csv('default_cc_train.csv', sep=',')\n",
    "\n",
    "# Read in new test data\n",
    "test_dat = pd.read_csv(test_data_file_string) \n",
    "\n",
    "test_predictions = bundle_predict(best_estimator, train_dat, test_dat)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-V7SGReTmZ1-"
   },
   "source": [
    "# Conclusion\n",
    "   Our best prediction accuracy was around 82-83%, our lowest measured prediction accuracy was about 80%. This is not a particularly large spread, especially considering the disparity in the time it takes for some models to train with the Grid Search to find the best hyper-parameters.\n",
    "\n",
    "   From our first week check-in period to the final due date of our report we streamlined the training and prediction process very well, but despite the feature engineering, and hyper parameter tuning we didn't see a particularly significant change in predictive power of our models. With the theoretical maximum being unknown, it's hard to determine where the ceiling would be in terms of predictive power of a Machine Learning Algorithm given this data set. Some outside research has shown some data scientists able to predict defaults with an accuracy of near 88%, but this involved the use of layers of Neural Networks, which is unfortunately beyond the scope of this project, and at least beyond my personal abilities in Machine Learning, not necessarily beyond the scope of everyone's abilities in our group. But time was also a slight limitation in this project as well. Given enough time, accuracy could likely be improved, but there is always a point of diminishing returns. Additionally, the concept of intelligibility must be considered in a project like this, if some black-box algorithm can predict someone's likelihood to default higher than ours, but the variables rely on the person's performance once they're already granted credit, it does little good to the bank or credit agency, other than giving them advanced warning, allowing them to revoke a person's credit sooner, rather than later.\n",
    "   \n",
    "  If we were to make recommendation to a crediting agency about the granting of credit, with respect to the debtors likelihood to default, it would be:\n",
    "  1. Consider the applicants marital status. Married people seem to default more often.\n",
    "  2. Consider the age of the applicant. Younger people are at higher risk of defaulting.\n",
    "  3. Establish a lower limit balance for applicants that would be considered risky. \n",
    "  4. Once granted credit, pay special attention to the ratio of payments to the amount owed on their balance. There will be some threshold were it's very unlikely that they will be able to pay you back.\n",
    "  5. Limit the number of months a debtor may be late on their payments to 3, like most credit agencies. At this point it's more likely that the person will default than pay you back. At this point you're only giving more money away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "b1aXPoNSmZxX",
    "3BuOPDwLmZzB",
    "02WV1WWbmZzP",
    "VdNaMJZdmZzY",
    "Jhls0dUSmZza",
    "38aTFc0vmZz7",
    "sXfkNDaVmZ0Y",
    "capqakdfmZ0u",
    "jhlfiO9nmZ0-",
    "M6DXsCuimZ1e"
   ],
   "name": "Project 1 (rough draft).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
